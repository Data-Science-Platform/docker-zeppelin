#!/bin/bash

export ZEPPELIN_NOTEBOOK_DIR="notebooks"   		# Where notebook at?

# export ZEPPELIN_JAVA_OPTS      		# Additional jvm options. for example, export ZEPPELIN_JAVA_OPTS="-Dspark.executor.memory=8g -Dspark.cores.max=16"
export ZEPPELIN_MEM="$ZEPPELIN_MEM"            		# Zeppelin jvm mem options Default -Xms1024m -Xmx1024m -XX:MaxPermSize=512m
# export ZEPPELIN_INTP_MEM       		# zeppelin interpreter process jvm mem options. Default -Xms1024m -Xmx1024m -XX:MaxPermSize=512m
# export ZEPPELIN_INTP_JAVA_OPTS 		# zeppelin interpreter process jvm options.
# export ZEPPELIN_SSL_PORT       		# ssl port (used when ssl environment variable is set to true)

#### Spark interpreter configuration ####

export SPARK_HOME=/usr/local/spark
export ZEPPELIN_SPARK_SCHEDULER_MODE="FAIR"
export SPARK_SUBMIT_OPTIONS="--driver-memory $ZEPPELIN_SPARK_DRIVER_MEMORY --conf spark.ui.port=$ZEPPELIN_SPARK_UI_PORT --conf spark.scheduler.mode=$ZEPPELIN_SPARK_SCHEDULER_MODE"

# Options read in YARN client mode
# export HADOOP_CONF_DIR         		# yarn-site.xml is located in configuration directory in HADOOP_CONF_DIR.
# Pyspark (supported with Spark 1.2.1 and above)
# To configure pyspark, you need to set spark distribution's path to 'spark.home' property in Interpreter setting screen in Zeppelin GUI
# export PYSPARK_PYTHON          		# path to the python command. must be the same path on the driver(Zeppelin) and all workers.
export PYSPARK_PYTHON="$ZEPPELIN_PYSPARK_PYTHON"
# export PYTHONPATH
